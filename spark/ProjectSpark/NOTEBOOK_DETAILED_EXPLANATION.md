# Gi·∫£i Th√≠ch Chi Ti·∫øt 3 Notebooks - ProjectSpark

> T√†i li·ªáu n√†y gi·∫£i th√≠ch t·ª´ng Cell, t·ª´ng d√≤ng code trong 3 file notebook ƒë·ªÉ b·∫°n c√≥ th·ªÉ tr√¨nh b√†y t·ª± tin.

---

## üìò Notebook 1: `1_ingest_and_explore.ipynb`

### üéØ M·ª•c Ti√™u
ƒê·ªçc d·ªØ li·ªáu JSON th√¥ t·ª´ GitHub Archive v√† kh√°m ph√° c·∫•u tr√∫c l·ªìng nhau ph·ª©c t·∫°p c·ªßa n√≥.

---

### üìù Cell 1: Kh·ªüi T·∫°o Spark Session

```python
from pyspark.sql import SparkSession
import os

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("ProjectSpark-Ingest") \
    .getOrCreate()

print("Spark Session Created: ", spark.version)
```

**Gi·∫£i th√≠ch t·ª´ng d√≤ng:**

1. `from pyspark.sql import SparkSession`
   - **Ch·ª©c nƒÉng**: Import class `SparkSession` t·ª´ th∆∞ vi·ªán PySpark
   - **L√Ω do**: SparkSession l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu ƒë·ªÉ l√†m vi·ªác v·ªõi Spark. N√≥ qu·∫£n l√Ω k·∫øt n·ªëi ƒë·∫øn Spark cluster v√† cung c·∫•p API ƒë·ªÉ ƒë·ªçc/x·ª≠ l√Ω d·ªØ li·ªáu
   
2. `import os`
   - **Ch·ª©c nƒÉng**: Import th∆∞ vi·ªán h·ªá ƒëi·ªÅu h√†nh Python
   - **L√Ω do**: ƒê·ªÉ c√≥ th·ªÉ thao t√°c v·ªõi ƒë∆∞·ªùng d·∫´n file, ki·ªÉm tra t·ªìn t·∫°i file, v.v.

3. `spark = SparkSession.builder`
   - **Ch·ª©c nƒÉng**: B·∫Øt ƒë·∫ßu x√¢y d·ª±ng (build) m·ªôt Spark Session m·ªõi
   - **L√Ω do**: S·ª≠ d·ª•ng Builder Pattern (m·∫´u thi·∫øt k·∫ø) ƒë·ªÉ c·∫•u h√¨nh Spark m·ªôt c√°ch linh ho·∫°t

4. `.appName("ProjectSpark-Ingest")`
   - **Ch·ª©c nƒÉng**: ƒê·∫∑t t√™n cho ·ª©ng d·ª•ng Spark
   - **L√Ω do**: T√™n n√†y hi·ªÉn th·ªã tr√™n Spark UI (giao di·ªán web gi√°m s√°t), gi√∫p ph√¢n bi·ªát c√°c job kh√°c nhau. "Ingest" nghƒ©a l√† "nh·∫≠p li·ªáu"

5. `.getOrCreate()`
   - **Ch·ª©c nƒÉng**: T·∫°o SparkSession m·ªõi HO·∫∂C l·∫•y session ƒëang t·ªìn t·∫°i (n·∫øu c√≥)
   - **L√Ω do**: Tr√°nh t·∫°o nhi·ªÅu session tr√πng l·∫∑p, ti·∫øt ki·ªám t√†i nguy√™n

6. `print("Spark Session Created: ", spark.version)`
   - **Ch·ª©c nƒÉng**: In ra phi√™n b·∫£n Spark ƒëang s·ª≠ d·ª•ng
   - **L√Ω do**: ƒê·ªÉ confirm r·∫±ng Spark ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng v√† bi·∫øt version (v√≠ d·ª•: 3.5.0)

**T·∫°i sao c·∫ßn Spark Session?**
- Spark Session l√† "c·ª≠a ng√µ" ƒë·ªÉ s·ª≠ d·ª•ng m·ªçi t√≠nh nƒÉng c·ªßa Spark
- N√≥ qu·∫£n l√Ω k·∫øt n·ªëi ƒë·∫øn cluster, ph√¢n b·ªï t√†i nguy√™n, v√† ƒëi·ªÅu ph·ªëi c√°c task

---

### üìù Cell 2: ƒê·ªçc D·ªØ Li·ªáu JSON

```python
input_path = "../data/raw/2015-01-01-15.json"

# Read JSON
df_raw = spark.read.json(input_path)

print(f"Loaded data from {input_path}")
```

**Gi·∫£i th√≠ch t·ª´ng d√≤ng:**

1. `input_path = "../data/raw/2015-01-01-15.json"`
   - **Ch·ª©c nƒÉng**: ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu th√¥
   - **L√Ω do**: S·ª≠ d·ª•ng bi·∫øn ƒë·ªÉ d·ªÖ thay ƒë·ªïi file ngu·ªìn sau n√†y
   - **√ù nghƒ©a `../`**: ƒêi l√™n 1 c·∫•p th∆∞ m·ª•c t·ª´ `src/` l√™n `ProjectSpark/`, sau ƒë√≥ v√†o `data/raw/`

2. `df_raw = spark.read.json(input_path)`
   - **Ch·ª©c nƒÉng**: ƒê·ªçc file JSON v√† t·∫°o DataFrame
   - **L√Ω do**: 
     - `spark.read` l√† DataFrameReader, c√¥ng c·ª• ƒë·ªÉ ƒë·ªçc nhi·ªÅu lo·∫°i file
     - `.json()` t·ª± ƒë·ªông ph√¢n t√≠ch c·∫•u tr√∫c JSON v√† suy lu·∫≠n schema (ki·ªÉu d·ªØ li·ªáu)
     - Spark r·∫•t th√¥ng minh: n√≥ t·ª± ƒë·ªông hi·ªÉu c·∫•u tr√∫c l·ªìng nhau (`actor`, `payload`, `commits[]`)
   - **K·∫øt qu·∫£**: `df_raw` l√† m·ªôt DataFrame - c·∫•u tr√∫c d·ªØ li·ªáu ph√¢n t√°n gi·ªëng b·∫£ng SQL

3. `print(f"Loaded data from {input_path}")`
   - **Ch·ª©c nƒÉng**: Th√¥ng b√°o ƒë√£ load xong
   - **L√Ω do**: Debug v√† feedback cho ng∆∞·ªùi d√πng

**T·∫°i sao d√πng `spark.read.json()` thay v√¨ `pandas.read_json()`?**
- Pandas ch·ªâ x·ª≠ l√Ω ƒë∆∞·ª£c tr√™n 1 m√°y, v·ªõi d·ªØ li·ªáu nh·ªè h∆°n RAM
- Spark c√≥ th·ªÉ x·ª≠ l√Ω h√†ng TB d·ªØ li·ªáu tr√™n nhi·ªÅu m√°y (distributed computing)

---

### üìù Cell 3: In C·∫•u Tr√∫c Schema

```python
df_raw.printSchema()
```

**Gi·∫£i th√≠ch:**

1. `df_raw.printSchema()`
   - **Ch·ª©c nƒÉng**: In ra c·∫•u tr√∫c d·ªØ li·ªáu (schema) d·∫°ng c√¢y
   - **L√Ω do**: ƒê·ªÉ hi·ªÉu d·ªØ li·ªáu l·ªìng bao nhi√™u c·∫•p, field n√†o quan tr·ªçng
   - **Output m·∫´u**:
     ```
     root
      |-- actor: struct (nullable = true)
      |    |-- login: string
      |-- payload: struct
      |    |-- commits: array
      |    |    |-- element: struct
      |    |    |    |-- author: struct
      |    |    |    |    |-- email: string
     ```

**L·ª£i √≠ch c·ªßa vi·ªác hi·ªÉu schema:**
- Bi·∫øt ƒë∆∞·ª£c d·ªØ li·ªáu n·∫±m ·ªü ƒë√¢u (v√≠ d·ª•: email n·∫±m ·ªü `payload.commits[].author.email`)
- L·∫≠p k·∫ø ho·∫°ch l√†m ph·∫≥ng (flatten) d·ªØ li·ªáu
- Tr√°nh l·ªói "field not found"

---

### üìù Cell 4-10: Kh√°m Ph√° D·ªØ Li·ªáu (Exploration)

C√°c cell ti·∫øp theo th·ª±c hi·ªán:

```python
# ƒê·∫øm s·ªë events
df_raw.count()

# Xem c√°c lo·∫°i event
df_raw.select("type").distinct().show()

# L·ªçc ch·ªâ l·∫•y PushEvent
df_push = df_raw.filter(df_raw.type == "PushEvent")

# Xem m·∫´u d·ªØ li·ªáu
df_push.select("id", "created_at", "actor.login").show(5)
```

**Gi·∫£i th√≠ch chi ti·∫øt:**

1. `df_raw.count()`
   - **Ch·ª©c nƒÉng**: ƒê·∫øm t·ªïng s·ªë d√≤ng (events)
   - **L√Ω do**: Bi·∫øt quy m√¥ d·ªØ li·ªáu (v√≠ d·ª•: 7000+ events)
   - **L∆∞u √Ω**: ƒê√¢y l√† m·ªôt **Action** trong Spark, s·∫Ω trigger th·ª±c thi t·∫•t c·∫£ c√°c transformation tr∆∞·ªõc ƒë√≥

2. `df_raw.select("type").distinct().show()`
   - **`select("type")`**: Ch·ªçn c·ªôt `type` (lo·∫°i event)
   - **`distinct()`**: Lo·∫°i b·ªè tr√πng l·∫∑p, ch·ªâ l·∫•y gi√° tr·ªã unique
   - **`show()`**: Hi·ªÉn th·ªã k·∫øt qu·∫£ ra m√†n h√¨nh
   - **L√Ω do**: ƒê·ªÉ bi·∫øt c√≥ bao nhi√™u lo·∫°i event (PushEvent, IssueEvent, WatchEvent,...)
   - **Transformation vs Action**: `select` v√† `distinct` l√† transformation (lazy, ch∆∞a ch·∫°y), `show()` l√† action (trigger ch·∫°y ngay)

3. `df_push = df_raw.filter(df_raw.type == "PushEvent")`
   - **Ch·ª©c nƒÉng**: L·ªçc ch·ªâ gi·ªØ l·∫°i c√°c event c√≥ type l√† "PushEvent"
   - **L√Ω do**: 
     - PushEvent ch·ª©a th√¥ng tin commits (m·ª•c ti√™u ph√¢n t√≠ch c·ªßa ta)
     - IssueEvent, WatchEvent kh√¥ng c√≥ commit data
   - **C√∫ ph√°p**: `df_raw.type` = truy c·∫≠p c·ªôt, `== "PushEvent"` = ƒëi·ªÅu ki·ªán l·ªçc
   - **L∆∞u √Ω**: ƒê√¢y l√† transformation, ch∆∞a ch·∫°y cho ƒë·∫øn khi g·∫∑p action

4. `df_push.select("id", "created_at", "actor.login").show(5)`
   - **`select(...)`**: Ch·ªçn nhi·ªÅu c·ªôt
   - **`"actor.login"`**: Truy c·∫≠p field l·ªìng nhau (nested field) - login n·∫±m trong struct `actor`
   - **`show(5)`**: Ch·ªâ hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu
   - **L√Ω do**: Xem m·∫´u d·ªØ li·ªáu ƒë·ªÉ ƒë·∫£m b·∫£o filter ƒë√∫ng

**T·∫°i sao ph·∫£i filter PushEvent?**
- Trong GitHub, c√≥ nhi·ªÅu lo·∫°i event nh∆∞ WatchEvent (star repo), IssueEvent (m·ªü issue), etc.
- Ch·ªâ PushEvent m·ªõi ch·ª©a array `commits` v·ªõi th√¥ng tin chi ti·∫øt v·ªÅ code changes
- N·∫øu kh√¥ng filter, s·∫Ω c√≥ nhi·ªÅu null values khi truy c·∫≠p `payload.commits`

---

## üìô Notebook 2: `2_flatten_data.ipynb`

### üéØ M·ª•c Ti√™u
Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON l·ªìng nhau th√†nh b·∫£ng ph·∫≥ng (flat table) ƒë·ªÉ d·ªÖ ph√¢n t√≠ch.

---

### üìù Cell 1: Import Functions

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, posexplode

spark = SparkSession.builder \
    .appName("ProjectSpark-Flatten") \
    .getOrCreate()
```

**Gi·∫£i th√≠ch c√°c import m·ªõi:**

1. `col`
   - **Ch·ª©c nƒÉng**: Tham chi·∫øu ƒë·∫øn m·ªôt c·ªôt trong DataFrame
   - **V√≠ d·ª•**: `col("actor.login")` = truy c·∫≠p field `login` trong struct `actor`
   - **L√Ω do import**: C√∫ ph√°p s·∫°ch h∆°n so v·ªõi `df["column_name"]`

2. `explode`
   - **Ch·ª©c nƒÉng**: "N·ªï" m·ªôt array th√†nh nhi·ªÅu d√≤ng
   - **V√≠ d·ª•**: 
     ```
     Tr∆∞·ªõc: [commit1, commit2, commit3] (1 d√≤ng)
     Sau explode: 
       - commit1 (d√≤ng 1)
       - commit2 (d√≤ng 2)  
       - commit3 (d√≤ng 3)
     ```
   - **L√Ω do**: M·ªói commit c·∫ßn 1 d√≤ng ri√™ng ƒë·ªÉ ph√¢n t√≠ch

3. `posexplode`
   - **Ch·ª©c nƒÉng**: Gi·ªëng `explode` nh∆∞ng th√™m c·ªôt `position` (v·ªã tr√≠ trong array)
   - **L√Ω do import**: D·ª± ph√≤ng n·∫øu c·∫ßn track th·ª© t·ª± commit

**T·∫°i sao c·∫ßn explode?**
- GitHub data c√≥ c·∫•u tr√∫c: 1 PushEvent ch·ª©a nhi·ªÅu commits
- Ta mu·ªën ph√¢n t√≠ch t·ª´ng commit ri√™ng l·∫ª
- Explode bi·∫øn 1 event (nhi·ªÅu commits) ‚Üí N rows (1 commit/row)

---

### üìù Cell 2: Load v√† Filter

```python
input_path = "../data/raw/2015-01-01-15.json"
df_raw = spark.read.json(input_path)

df_push = df_raw.filter(df_raw.type == "PushEvent")
print(f"Push Events to process: {df_push.count()}")
```

**Gi·∫£i th√≠ch:**
- T∆∞∆°ng t·ª± Notebook 1
- Kh√°c bi·ªát: In ra s·ªë l∆∞·ª£ng PushEvent ƒë·ªÉ bi·∫øt scope c√¥ng vi·ªác
- V√≠ d·ª• output: "Push Events to process: 5815"

---

### üìù Cell 3: B∆∞·ªõc 1 - Explode Commits Array

```python
df_flat = (
    df_push
    .select(
        col("id").alias("event_id"),
        col("created_at").alias("event_time"),
        col("actor.login").alias("actor_login"),
        col("repo.name").alias("repo_name"),
        col("payload.head").alias("head_sha"),
        # explode creates a new row for each commit in the array
        explode(col("payload.commits")).alias("commit")
    )
)

df_flat.printSchema()
```

**Gi·∫£i th√≠ch t·ª´ng ph·∫ßn:**

1. `df_flat = ( ... )`
   - **C√∫ ph√°p**: D·∫•u ngo·∫∑c cho ph√©p vi·∫øt code nhi·ªÅu d√≤ng d·ªÖ ƒë·ªçc
   - **Pattern**: Method chaining - g·ªçi nhi·ªÅu method li√™n ti·∫øp

2. `col("id").alias("event_id")`
   - **`col("id")`**: Tham chi·∫øu ƒë·∫øn c·ªôt `id`
   - **`.alias("event_id")`**: ƒê·ªïi t√™n c·ªôt th√†nh `event_id`
   - **L√Ω do ƒë·ªïi t√™n**: ƒê·ªÉ r√µ nghƒ©a h∆°n ("event_id" r√µ r√†ng h∆°n "id")

3. `col("actor.login").alias("actor_login")`
   - **C√∫ ph√°p `actor.login`**: Truy c·∫≠p field l·ªìng nhau
   - **K·∫øt qu·∫£**: L·∫•y username c·ªßa ng∆∞·ªùi push code
   - **L√Ω do flatten**: T·ª´ `actor.login` (nested) ‚Üí `actor_login` (flat column)

4. `explode(col("payload.commits")).alias("commit")`
   - **`col("payload.commits")`**: Truy c·∫≠p array commits
   - **`explode(...)`**: M·ªói element trong array ‚Üí 1 row m·ªõi
   - **`.alias("commit")`**: ƒê·∫∑t t√™n c·ªôt m·ªõi l√† `commit` (ch·ª©a struct c·ªßa t·ª´ng commit)
   - **V√≠ d·ª• minh h·ªça**:
     ```
     Tr∆∞·ªõc explode (1 row):
     event_id | commits
     12345    | [{sha: "abc", msg: "fix"}, {sha: "def", msg: "add"}]
     
     Sau explode (2 rows):
     event_id | commit
     12345    | {sha: "abc", msg: "fix"}
     12345    | {sha: "def", msg: "add"}
     ```

**T·∫°i sao c·∫ßn b∆∞·ªõc n√†y tr∆∞·ªõc?**
- Kh√¥ng th·ªÉ flatten to√†n b·ªô c√πng l√∫c v√¨ `commits` l√† array
- Ph·∫£i explode array tr∆∞·ªõc, sau ƒë√≥ m·ªõi flatten struct

---

### üìù Cell 4: B∆∞·ªõc 2 - Flatten Struct

```python
df_final = df_flat.select(
    col("event_id"),
    col("event_time"),
    col("actor_login"),
    col("repo_name"),
    col("commit.sha").alias("commit_sha"),
    col("commit.author.name").alias("author_name"),
    col("commit.author.email").alias("author_email"),
    col("commit.message").alias("commit_message")
)

df_final.show(5, truncate=False)
```

**Gi·∫£i th√≠ch t·ª´ng field:**

1. `col("commit.sha").alias("commit_sha")`
   - **`commit.sha`**: Truy c·∫≠p field `sha` b√™n trong struct `commit`
   - **L√Ω do**: M√£ SHA l√† ID duy nh·∫•t c·ªßa commit
   - **K·∫øt qu·∫£**: T·ª´ c·∫•u tr√∫c `commit.sha` ‚Üí c·ªôt ph·∫≥ng `commit_sha`

2. `col("commit.author.name").alias("author_name")`
   - **ƒê·ªô s√¢u nested**: 2 levels (commit ‚Üí author ‚Üí name)
   - **Ch·ª©c nƒÉng**: L·∫•y t√™n ng∆∞·ªùi vi·∫øt commit
   - **Kh√°c v·ªõi actor_login**: 
     - `actor_login` = GitHub username (ng∆∞·ªùi push)
     - `author_name` = t√™n trong commit (ng∆∞·ªùi vi·∫øt code, c√≥ th·ªÉ kh√°c ng∆∞·ªùi push)

3. `col("commit.author.email").alias("author_email")`
   - **Ch·ª©c nƒÉng**: Email c·ªßa author
   - **L√Ω do quan tr·ªçng**: ƒê·ªÉ mapping author ‚Üí organization (gmail.com, company.com,...)

4. `col("commit.message").alias("commit_message")`
   - **Ch·ª©c nƒÉng**: N·ªôi dung commit message
   - **L√Ω do**: ƒê·ªÉ ph√¢n t√≠ch keyword ("fix", "add", "refactor",...)

5. `show(5, truncate=False)`
   - **`truncate=False`**: Hi·ªÉn th·ªã to√†n b·ªô n·ªôi dung, kh√¥ng c·∫Øt ng·∫Øn
   - **L√Ω do**: Commit message th∆∞·ªùng d√†i, c·∫ßn xem ƒë·∫ßy ƒë·ªß

**T·∫°i sao ph·∫£i l√†m 2 b∆∞·ªõc (Explode r·ªìi Flatten)?**
- B∆∞·ªõc 1: Bi·∫øn array ‚Üí rows (vertical expansion)
- B∆∞·ªõc 2: Bi·∫øn struct ‚Üí columns (horizontal expansion)
- Kh√¥ng th·ªÉ l√†m 1 b∆∞·ªõc v√¨ nested structure ph·ª©c t·∫°p

---

### üìù Cell 5: L∆∞u K·∫øt Qu·∫£

```python
output_csv = "../data/processed/github_commits_flat.csv"
output_parquet = "../data/processed/github_commits_flat.parquet"

# Write CSV (coalesce(1) to get single file for easy handling)
# Added quoteAll=True to handle commas in commit messages
df_final.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .option("quoteAll", "true") \
    .csv(output_csv)

# Write Parquet
df_final.write.mode("overwrite").parquet(output_parquet)

print("Data saved successfully!")
```

**Gi·∫£i th√≠ch t·ª´ng d√≤ng:**

1. `df_final.coalesce(1)`
   - **Ch·ª©c nƒÉng**: G·ªôp t·∫•t c·∫£ partitions th√†nh 1 file
   - **L√Ω do**: 
     - Spark m·∫∑c ƒë·ªãnh t·∫°o nhi·ªÅu file (distributed)
     - Ta c·∫ßn 1 file CSV duy nh·∫•t ƒë·ªÉ d·ªÖ xem v√† share
   - **Trade-off**: Ch·∫≠m h∆°n v·ªõi d·ªØ li·ªáu l·ªõn (do ph·∫£i merge)

2. `.write.mode("overwrite")`
   - **`write`**: DataFrameWriter - c√¥ng c·ª• ghi file
   - **`mode("overwrite")`**: N·∫øu file ƒë√£ t·ªìn t·∫°i, ghi ƒë√®
   - **C√°c mode kh√°c**: 
     - `"append"`: Th√™m v√†o
     - `"error"`: B√°o l·ªói n·∫øu file t·ªìn t·∫°i
     - `"ignore"`: B·ªè qua n·∫øu t·ªìn t·∫°i

3. `.option("header", "true")`
   - **Ch·ª©c nƒÉng**: Ghi header (t√™n c·ªôt) v√†o d√≤ng ƒë·∫ßu ti√™n c·ªßa CSV
   - **L√Ω do**: ƒê·ªÉ Excel, Pandas ƒë·ªçc ƒë∆∞·ª£c t√™n c·ªôt

4. `.option("quoteAll", "true")`
   - **Ch·ª©c nƒÉng**: ƒê·∫∑t d·∫•u ngo·∫∑c k√©p cho T·∫§T C·∫¢ gi√° tr·ªã
   - **L√Ω do**: Commit message ch·ª©a d·∫•u ph·∫©y, xu·ªëng d√≤ng ‚Üí d·ªÖ l√†m l·ªói CSV
   - **V√≠ d·ª•**: `"Fix bug, add test"` ‚Üí N·∫øu kh√¥ng quote, CSV nghƒ© l√† 2 c·ªôt

5. `.csv(output_csv)`
   - **Ch·ª©c nƒÉng**: Ghi ra ƒë·ªãnh d·∫°ng CSV
   - **L∆∞u √Ω**: Spark ghi v√†o th∆∞ m·ª•c (folder), b√™n trong c√≥ file `part-xxxxx.csv`

6. `.parquet(output_parquet)`
   - **Parquet**: ƒê·ªãnh d·∫°ng columnar, n√©n t·ªët, ƒë·ªçc nhanh
   - **L√Ω do l∆∞u c·∫£ CSV v√† Parquet**:
     - CSV: Con ng∆∞·ªùi ƒë·ªçc, Excel m·ªü ƒë∆∞·ª£c
     - Parquet: Spark ƒë·ªçc nhanh h∆°n 10-100x, ti·∫øt ki·ªám dung l∆∞·ª£ng

**T·∫°i sao c·∫ßn quoteAll?**
Xem v√≠ d·ª•:
```
# Kh√¥ng quote:
Author,Message
John,Fix bug, add test
‚Üí L·ªói: CSV parser nghƒ© c√≥ 3 c·ªôt (Author, Message="Fix bug", Unknown="add test")

# C√≥ quoteAll:
Author,Message
"John","Fix bug, add test"
‚Üí ƒê√∫ng: 2 c·ªôt ƒë∆∞·ª£c b·∫£o v·ªá b·ªüi d·∫•u ngo·∫∑c k√©p
```

---

## üìó Notebook 3: `3_analyze_dashboard.ipynb`

### üéØ M·ª•c Ti√™u
Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ flatten ƒë·ªÉ t·∫°o insights cho b√°o c√°o.

---

### üìù Cell 1: Import v√† Kh·ªüi T·∫°o

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, desc, hour

spark = SparkSession.builder \
    .appName("ProjectSpark-Analyze") \
    .getOrCreate()
```

**Gi·∫£i th√≠ch Import M·ªõi:**

1. `count`
   - **Ch·ª©c nƒÉng**: H√†m ƒë·∫øm s·ªë l∆∞·ª£ng
   - **V√≠ d·ª•**: `count("actor_login")` = ƒë·∫øm s·ªë l∆∞·ª£ng actor
   
2. `desc`
   - **Ch·ª©c nƒÉng**: S·∫Øp x·∫øp gi·∫£m d·∫ßn (descending)
   - **V√≠ d·ª•**: `.orderBy(desc("count"))` = s·∫Øp x·∫øp t·ª´ l·ªõn ‚Üí nh·ªè

3. `hour`
   - **Ch·ª©c nƒÉng**: Tr√≠ch xu·∫•t gi·ªù t·ª´ timestamp
   - **V√≠ d·ª•**: `hour("2015-01-01T15:30:00Z")` ‚Üí `15`
   - **L√Ω do**: ƒê·ªÉ ph√¢n t√≠ch ho·∫°t ƒë·ªông theo gi·ªù trong ng√†y

---

### üìù Cell 2: Load D·ªØ Li·ªáu

```python
input_parquet = "../data/processed/github_commits_flat.parquet"
df = spark.read.parquet(input_parquet)
df.createOrReplaceTempView("commits")

print(f"Total Commits Analyzed: {df.count()}")
```

**Gi·∫£i th√≠ch:**

1. `spark.read.parquet(input_parquet)`
   - **Ch·ª©c nƒÉng**: ƒê·ªçc file Parquet
   - **L√Ω do d√πng Parquet**: Nhanh h∆°n CSV 10-100 l·∫ßn
   - **Schema**: Parquet t·ª± l∆∞u schema, kh√¥ng c·∫ßn suy lu·∫≠n

2. `df.createOrReplaceTempView("commits")`
   - **Ch·ª©c nƒÉng**: T·∫°o m·ªôt "view" t·∫°m th·ªùi t√™n l√† `commits`
   - **L√Ω do**: ƒê·ªÉ c√≥ th·ªÉ d√πng SQL query
   - **V√≠ d·ª•**: Sau d√≤ng n√†y c√≥ th·ªÉ d√πng `spark.sql("SELECT * FROM commits")`
   - **Temp view**: Ch·ªâ t·ªìn t·∫°i trong session hi·ªán t·∫°i, kh√¥ng l∆∞u v√†o database

3. `df.count()`
   - **K·∫øt qu·∫£**: In ra "Total Commits Analyzed: 10109"
   - **L√Ω do**: ƒê·ªÉ bi·∫øt quy m√¥ d·ªØ li·ªáu ph√¢n t√≠ch

---

### üìù Cell 3: Top 10 Contributors

```python
top_contributors = df.groupBy("actor_login") \
    .count() \
    .orderBy(desc("count")) \
    .limit(10)

top_contributors.show()
```

**Gi·∫£i th√≠ch t·ª´ng ph·∫ßn:**

1. `df.groupBy("actor_login")`
   - **Ch·ª©c nƒÉng**: Nh√≥m c√°c row c√≥ c√πng `actor_login`
   - **Gi·ªëng SQL**: `GROUP BY actor_login`
   - **K·∫øt qu·∫£**: T·∫°o GroupedData object

2. `.count()`
   - **Ch·ª©c nƒÉng**: ƒê·∫øm s·ªë row trong m·ªói group
   - **K·∫øt qu·∫£**: T·∫°o c·ªôt m·ªõi t√™n `count`
   - **V√≠ d·ª•**:
     ```
     actor_login | count
     mirror-updates | 413
     KenanSulayman | 80
     ```

3. `.orderBy(desc("count"))`
   - **Ch·ª©c nƒÉng**: S·∫Øp x·∫øp theo c·ªôt `count` gi·∫£m d·∫ßn
   - **`desc()`**: T·ª´ cao xu·ªëng th·∫•p (ng∆∞·ªùi push nhi·ªÅu nh·∫•t l√™n ƒë·∫ßu)
   - **Gi·ªëng SQL**: `ORDER BY count DESC`

4. `.limit(10)`
   - **Ch·ª©c nƒÉng**: Ch·ªâ l·∫•y 10 row ƒë·∫ßu
   - **L√Ω do**: Top 10 contributor, kh√¥ng c·∫ßn to√†n b·ªô

5. `.show()`
   - **Ch·ª©c nƒÉng**: Hi·ªÉn th·ªã k·∫øt qu·∫£ d·∫°ng b·∫£ng
   - **L√† Action**: Trigger Spark th·ª±c thi t·∫•t c·∫£ transformation

**T·∫°i sao ph√¢n t√≠ch n√†y quan tr·ªçng?**
- Bi·∫øt ai l√† ng∆∞·ªùi ƒë√≥ng g√≥p nhi·ªÅu nh·∫•t
- Ph√°t hi·ªán bot account (v√≠ d·ª•: mirror-updates c√≥ 413 commits)
- Hi·ªÉu ƒë∆∞·ª£c community structure c·ªßa d·ª± √°n

---

### üìù Cell 4: Top 10 Repositories

```python
top_repos = df.groupBy("repo_name") \
    .count() \
    .orderBy(desc("count")) \
    .limit(10)

top_repos.show(truncate=False)
```

**Gi·∫£i th√≠ch:**
- Logic t∆∞∆°ng t·ª± Cell 3, ch·ªâ kh√°c:
  - Group theo `repo_name` thay v√¨ `actor_login`
  - `truncate=False`: Hi·ªÉn th·ªã t√™n repo ƒë·∫ßy ƒë·ªß (c√≥ th·ªÉ d√†i nh∆∞ `sakai-mirror/melete`)

**Insight t·ª´ k·∫øt qu·∫£:**
- `sakai-mirror/melete` c√≥ 235 commits ‚Üí Repo r·∫•t active
- C√°c repo mirror th∆∞·ªùng c√≥ s·ªë commit cao (automated)

---

### üìù Cell 5: SQL Analysis - T√¨m Commits Ch·ª©a "fix"

```python
fixes = spark.sql("""
    SELECT actor_login, repo_name, commit_message
    FROM commits
    WHERE lower(commit_message) LIKE '%fix%'
    LIMIT 5
""")
fixes.show(truncate=False)
```

**Gi·∫£i th√≠ch t·ª´ng ph·∫ßn:**

1. `spark.sql("...")`
   - **Ch·ª©c nƒÉng**: Ch·∫°y SQL query thu·∫ßn t√∫y tr√™n Spark
   - **L√Ω do**: Ng∆∞·ªùi quen SQL c√≥ th·ªÉ d√πng ngay, kh√¥ng c·∫ßn h·ªçc DataFrame API

2. `FROM commits`
   - **Ch·ª©c nƒÉng**: Query t·ª´ view `commits` ƒë√£ t·∫°o ·ªü Cell 2
   - **L∆∞u √Ω**: Ph·∫£i g·ªçi `createOrReplaceTempView()` tr∆∞·ªõc

3. `lower(commit_message)`
   - **Ch·ª©c nƒÉng**: Chuy·ªÉn chu·ªói v·ªÅ ch·ªØ th∆∞·ªùng
   - **L√Ω do**: ƒê·ªÉ search kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng
   - **V√≠ d·ª•**: "Fix bug" v√† "FIX BUG" ƒë·ªÅu match

4. `LIKE '%fix%'`
   - **Ch·ª©c nƒÉng**: Pattern matching
   - **`%`**: Wildcard = b·∫•t k·ª≥ k√Ω t·ª± n√†o
   - **`%fix%`**: Ch·ª©a "fix" ·ªü b·∫•t k·ª≥ ƒë√¢u
   - **V√≠ d·ª• match**: "Fix bug", "bugfix", "fixing issue"

5. `LIMIT 5`
   - **Ch·ª©c nƒÉng**: Ch·ªâ l·∫•y 5 k·∫øt qu·∫£ ƒë·∫ßu
   - **L√Ω do**: Demo, kh√¥ng c·∫ßn to√†n b·ªô

**T·∫°i sao ph√¢n t√≠ch keyword trong commit message?**
- Hi·ªÉu developer behavior (bao nhi√™u % commit l√† fix bug?)
- Ph√°t hi·ªán pattern (commits c√≥ "fix" th∆∞·ªùng nh·ªè h∆°n "feature")
- Quality assessment (nhi·ªÅu "fix" ‚Üí code quality th·∫•p?)

---

## üîë C√°c Kh√°i Ni·ªám Quan Tr·ªçng

### 1. Transformation vs Action

**Transformation (Lazy)**
- Ch·ªâ t·∫°o execution plan, ch∆∞a ch·∫°y th·∫≠t
- V√≠ d·ª•: `select`, `filter`, `groupBy`, `orderBy`
- M·ª•c ƒë√≠ch: Spark t·ªëi ∆∞u h√≥a query plan

**Action (Eager)**
- Trigger th·ª±c thi to√†n b·ªô plan
- V√≠ d·ª•: `count()`, `show()`, `write()`
- L√∫c n√†y Spark m·ªõi th·ª±c s·ª± x·ª≠ l√Ω d·ªØ li·ªáu

**V√≠ d·ª• minh h·ªça:**
```python
df2 = df.filter(col("type") == "PushEvent")  # Lazy, ch∆∞a ch·∫°y
df3 = df2.select("id", "actor.login")        # Lazy, ch∆∞a ch·∫°y
df3.show()                                    # Action, B√ôM! Spark ch·∫°y h·∫øt
```

### 2. Nested vs Flat Structure

**Nested (L·ªìng nhau):**
```json
{
  "actor": {
    "login": "john",
    "id": 123
  },
  "payload": {
    "commits": [
      {"sha": "abc", "message": "fix"},
      {"sha": "def", "message": "add"}
    ]
  }
}
```

**Flat (Ph·∫≥ng):**
```
actor_login | commit_sha | commit_message
john        | abc        | fix
john        | def        | add
```

**L√Ω do c·∫ßn flatten:**
- SQL/BI tools ch·ªâ hi·ªÉu b·∫£ng ph·∫≥ng
- D·ªÖ ph√¢n t√≠ch, query, visualize

### 3. Schema-on-Read

- **ƒê·ªãnh nghƒ©a**: Spark suy lu·∫≠n schema khi ƒë·ªçc file
- **Kh√°c v·ªõi Database**: DB c·∫ßn ƒë·ªãnh nghƒ©a schema tr∆∞·ªõc (schema-on-write)
- **∆Øu ƒëi·ªÉm**: Linh ho·∫°t v·ªõi d·ªØ li·ªáu thay ƒë·ªïi
- **Nh∆∞·ª£c ƒëi·ªÉm**: C√≥ th·ªÉ suy lu·∫≠n sai ki·ªÉu d·ªØ li·ªáu

---

## üìä Lu·ªìng X·ª≠ L√Ω T·ªïng Quan

```mermaid
graph LR
    A[Raw JSON] -->|Notebook 1| B[Explore Schema]
    B -->|Notebook 2| C[Explode Array]
    C --> D[Flatten Struct]
    D --> E[Save CSV/Parquet]
    E -->|Notebook 3| F[GroupBy Analysis]
    E -->|Notebook 3| G[SQL Query]
    F --> H[Insights]
    G --> H
```

---

## üí° Tips Tr√¨nh B√†y

1. **Lu√¥n gi·∫£i th√≠ch "T·∫°i sao"**
   - T·∫°i sao d√πng Spark? ‚Üí Big Data
   - T·∫°i sao explode? ‚Üí Array ‚Üí Rows
   - T·∫°i sao Parquet? ‚Üí Performance

2. **D√πng v√≠ d·ª• c·ª• th·ªÉ**
   - Kh√¥ng n√≥i "x·ª≠ l√Ω d·ªØ li·ªáu"
   - N√≥i "bi·∫øn 1 event c√≥ 3 commits th√†nh 3 rows"

3. **So s√°nh v·ªõi c√¥ng c·ª• quen thu·ªôc**
   - DataFrame gi·ªëng Excel table
   - `groupBy()` gi·ªëng Pivot Table
   - TempView gi·ªëng SQL View

4. **Highlight s·ªë li·ªáu**
   - "X·ª≠ l√Ω 16,000+ commits"
   - "Gi·∫£m t·ª´ 5 levels nested ‚Üí 1 flat table"
   - "Parquet nhanh h∆°n CSV 100x"

---

## üéì C√¢u H·ªèi Th∆∞·ªùng G·∫∑p

**Q: T·∫°i sao kh√¥ng d√πng Pandas?**
**A:** Pandas ch·ªâ x·ª≠ l√Ω tr√™n 1 m√°y, b·ªã gi·ªõi h·∫°n RAM. Spark ph√¢n t√°n tr√™n nhi·ªÅu m√°y, x·ª≠ l√Ω ƒë∆∞·ª£c TB data.

**Q: Spark SQL kh√°c g√¨ SQL th∆∞·ªùng?**
**A:** Gi·ªëng nhau v·ªÅ c√∫ ph√°p, kh√°c l√† Spark SQL ch·∫°y distributed, scale ƒë∆∞·ª£c v·ªõi big data.

**Q: T·∫°i sao ph·∫£i explode r·ªìi m·ªõi flatten?**
**A:** V√¨ c·∫•u tr√∫c 3 levels: Struct ‚Üí Array ‚Üí Struct. Ph·∫£i x·ª≠ l√Ω Array (explode) tr∆∞·ªõc, sau m·ªõi x·ª≠ l√Ω Struct (flatten).

**Q: coalesce(1) c√≥ l√†m ch·∫≠m kh√¥ng?**
**A:** C√≥, v√¨ ph·∫£i merge partitions. Nh∆∞ng v·ªõi demo dataset nh·ªè (16K rows) th√¨ kh√¥ng ·∫£nh h∆∞·ªüng.

---

*Ch√∫c b·∫°n tr√¨nh b√†y th√†nh c√¥ng! üöÄ*
